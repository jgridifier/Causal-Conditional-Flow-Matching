{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Conditional Flow Matching: A Deep Dive Tutorial\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of the Causal Conditional Flow Matching (C-CFM) framework. We'll explore each component in detail, examining the mathematical foundations and visualizing intermediate results.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Introduction to Flow Matching**\n",
    "2. **CTree-Lite: Statistical Regime Detection**\n",
    "3. **Data Processing and Causal Discovery**\n",
    "4. **The Masked Velocity Network**\n",
    "5. **Training with Optimal Transport Paths**\n",
    "6. **Scenario Generation and Stress Testing**\n",
    "7. **Full Pipeline Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2, rankdata\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Flow Matching\n",
    "\n",
    "Flow Matching is a framework for training continuous normalizing flows (CNFs) without solving ODEs during training. The key insight is that we can learn a velocity field $v_\\theta(x, t)$ that transports samples from a simple prior (e.g., Gaussian noise) to the data distribution.\n",
    "\n",
    "### The Optimal Transport Path\n",
    "\n",
    "Given:\n",
    "- $x_0 \\sim \\mathcal{N}(0, I)$ - noise\n",
    "- $x_1 \\sim p_{data}$ - real data\n",
    "\n",
    "The OT interpolation path is:\n",
    "$$x_t = (1-t) \\cdot x_0 + t \\cdot x_1$$\n",
    "\n",
    "And the target velocity along this path is:\n",
    "$$u_t = \\frac{dx_t}{dt} = x_1 - x_0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the OT path\n",
    "def visualize_ot_path(n_samples=5, n_steps=10):\n",
    "    \"\"\"Visualize how samples flow from noise to data.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Generate noise and \"data\" (for visualization, use 2D)\n",
    "    x_0 = np.random.randn(n_samples, 2)  # Noise\n",
    "    x_1 = np.random.randn(n_samples, 2) * 0.5 + np.array([3, 3])  # \"Data\"\n",
    "    \n",
    "    # Time steps\n",
    "    t_values = np.linspace(0, 1, n_steps)\n",
    "    \n",
    "    # Plot paths\n",
    "    ax = axes[0]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        path = [(1-t) * x_0[i] + t * x_1[i] for t in t_values]\n",
    "        path = np.array(path)\n",
    "        ax.plot(path[:, 0], path[:, 1], 'o-', color=colors[i], alpha=0.7, markersize=4)\n",
    "        ax.scatter([x_0[i, 0]], [x_0[i, 1]], s=100, c='blue', marker='o', zorder=5)\n",
    "        ax.scatter([x_1[i, 0]], [x_1[i, 1]], s=100, c='red', marker='x', zorder=5)\n",
    "    ax.set_title('OT Interpolation Paths\\n(Blue=Noise, Red=Data)')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    \n",
    "    # Plot velocity field\n",
    "    ax = axes[1]\n",
    "    for i in range(n_samples):\n",
    "        velocity = x_1[i] - x_0[i]  # Constant along path\n",
    "        for t in t_values[::2]:\n",
    "            x_t = (1-t) * x_0[i] + t * x_1[i]\n",
    "            ax.arrow(x_t[0], x_t[1], velocity[0]*0.1, velocity[1]*0.1,\n",
    "                    head_width=0.1, head_length=0.05, fc=colors[i], ec=colors[i], alpha=0.5)\n",
    "    ax.set_title('Target Velocity Field\\n$u_t = x_1 - x_0$')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    \n",
    "    # Plot marginals at different times\n",
    "    ax = axes[2]\n",
    "    x_0_many = np.random.randn(500, 2)\n",
    "    x_1_many = np.random.randn(500, 2) * 0.5 + np.array([3, 3])\n",
    "    \n",
    "    for t, color in zip([0, 0.5, 1], ['blue', 'purple', 'red']):\n",
    "        x_t = (1-t) * x_0_many + t * x_1_many\n",
    "        ax.scatter(x_t[:, 0], x_t[:, 1], alpha=0.3, s=5, c=color, label=f't={t}')\n",
    "    ax.legend()\n",
    "    ax.set_title('Distribution Evolution\\nt=0 (Blue) → t=1 (Red)')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ot_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CTree-Lite: Statistical Regime Detection\n",
    "\n",
    "Unlike standard decision trees (CART) that split based on impurity reduction, CTree-Lite uses **statistical significance testing** to determine splits. This is crucial for financial data where noise can lead to overfitting.\n",
    "\n",
    "### The Strasser-Weber Framework\n",
    "\n",
    "For each candidate variable $X_j$, we test:\n",
    "$$H_0: X_j \\perp Y \\text{ (independence)}$$\n",
    "\n",
    "The test statistic is computed using rank transformations for robustness:\n",
    "\n",
    "1. Compute ranks: $h(X_j) = \\text{rank}(X_j)$\n",
    "2. Linear statistic: $T = Y^\\top \\cdot h(X_j)$\n",
    "3. Standardize: $S = (T - \\mu_T)^\\top \\Sigma_T^{-1} (T - \\mu_T)$\n",
    "4. P-value: $p = 1 - F_{\\chi^2}(S)$\n",
    "\n",
    "We only split if $p < \\alpha$ (typically 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.ctree import CTree\n",
    "\n",
    "# Generate data with clear regime structure\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "# Create features\n",
    "X = np.random.randn(n, 3)\n",
    "X[:, 0] = np.linspace(-2, 2, n)  # Sorted for visualization\n",
    "\n",
    "# Create response that depends on X[:, 0]\n",
    "# Regime 1: X[:, 0] < 0 → Y centered at -1\n",
    "# Regime 2: X[:, 0] >= 0 → Y centered at 1\n",
    "Y = np.where(X[:, 0] < 0, -1 + 0.3*np.random.randn(n), 1 + 0.3*np.random.randn(n))\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# Fit CTree\n",
    "ctree = CTree(alpha=0.05, min_split=20)\n",
    "ctree.fit(X, Y, feature_names=['X1', 'X2', 'X3'])\n",
    "\n",
    "# Get predictions\n",
    "regimes = ctree.predict(X)\n",
    "\n",
    "print(\"CTree-Lite Results:\")\n",
    "print(f\"Number of regimes: {ctree.n_regimes}\")\n",
    "print(f\"\\nTree structure:\")\n",
    "print(ctree.print_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regime detection\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Data colored by true regime\n",
    "ax = axes[0]\n",
    "true_regime = (X[:, 0] >= 0).astype(int)\n",
    "scatter = ax.scatter(X[:, 0], Y.ravel(), c=true_regime, cmap='coolwarm', alpha=0.5)\n",
    "ax.axvline(x=0, color='black', linestyle='--', label='True boundary')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Data with True Regimes')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Data colored by detected regime\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(X[:, 0], Y.ravel(), c=regimes, cmap='coolwarm', alpha=0.5)\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title(f'CTree Detected Regimes ({ctree.n_regimes} found)')\n",
    "\n",
    "# Plot 3: Show independence test p-values\n",
    "ax = axes[2]\n",
    "# Compute p-values for each variable\n",
    "p_values = [ctree._test_independence(X[:, j], Y) for j in range(3)]\n",
    "bars = ax.bar(['X1', 'X2', 'X3'], p_values, color=['red' if p < 0.05 else 'blue' for p in p_values])\n",
    "ax.axhline(y=0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "ax.set_ylabel('P-value')\n",
    "ax.set_title('Independence Test P-values\\n(Red = Significant)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nP-values: X1={p_values[0]:.4f}, X2={p_values[1]:.4f}, X3={p_values[2]:.4f}\")\n",
    "print(f\"Only X1 is significant (p < 0.05), so it's used for splitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Processing and Causal Discovery\n",
    "\n",
    "The ETL pipeline performs several critical steps:\n",
    "\n",
    "1. **Stationarity Check** (ADF test) - Non-stationary series are differenced\n",
    "2. **Imputation** (Cubic spline) - Missing values are interpolated\n",
    "3. **Normalization** (Z-score) - Required for stable training\n",
    "4. **Causal Discovery** (LiNGAM) - Determines variable ordering\n",
    "\n",
    "### Why Causal Ordering Matters\n",
    "\n",
    "In our framework:\n",
    "- **Slow variables** (macro: GDP, CPI) are upstream (cause)\n",
    "- **Fast variables** (market: VIX, returns) are downstream (effect)\n",
    "\n",
    "The masked network ensures slow variables cannot be influenced by fast variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.etl import DataProcessor, validate_for_ode\n",
    "from examples.data_fetcher import create_sample_dataset\n",
    "\n",
    "# Load sample data\n",
    "X_raw, fast_vars, slow_vars = create_sample_dataset()\n",
    "\n",
    "print(f\"Raw data shape: {X_raw.shape}\")\n",
    "print(f\"\\nFast variables (market): {fast_vars}\")\n",
    "print(f\"\\nSlow variables (macro): {slow_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "processor = DataProcessor(\n",
    "    adf_threshold=0.05,\n",
    "    ctree_alpha=0.10,\n",
    "    ctree_min_split=50\n",
    ")\n",
    "\n",
    "topology = processor.fit_transform(\n",
    "    X_raw,\n",
    "    fast_vars=fast_vars,\n",
    "    slow_vars=slow_vars\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Topology Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Processed samples: {topology.X_processed.shape[0]}\")\n",
    "print(f\"Number of variables: {topology.X_processed.shape[1]}\")\n",
    "print(f\"Detected regimes: {topology.n_regimes}\")\n",
    "print(f\"\\nCausal ordering (first 10):\")\n",
    "for i, name in enumerate(topology.variable_names[:10]):\n",
    "    var_type = \"SLOW\" if i < len(topology.slow_indices) else \"FAST\"\n",
    "    print(f\"  {i}: {name} ({var_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data processing effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Raw data distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(X_raw[:, 0], bins=50, alpha=0.7, label='Var 0 (raw)')\n",
    "ax.hist(X_raw[:, -1], bins=50, alpha=0.7, label='Var -1 (raw)')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Raw Data Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Processed (normalized) data\n",
    "ax = axes[0, 1]\n",
    "ax.hist(topology.X_processed[:, 0], bins=50, alpha=0.7, label='Var 0 (normalized)')\n",
    "ax.hist(topology.X_processed[:, -1], bins=50, alpha=0.7, label='Var -1 (normalized)')\n",
    "ax.set_xlabel('Value (Z-score)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Normalized Data Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Regime distribution\n",
    "ax = axes[1, 0]\n",
    "unique, counts = np.unique(topology.regimes, return_counts=True)\n",
    "ax.bar(unique, counts, color=plt.cm.viridis(np.linspace(0, 1, len(unique))))\n",
    "ax.set_xlabel('Regime')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Regime Distribution ({topology.n_regimes} regimes)')\n",
    "\n",
    "# Plot 4: Correlation heatmap (showing causal structure)\n",
    "ax = axes[1, 1]\n",
    "corr = np.corrcoef(topology.X_processed.T)\n",
    "im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix (Causal Order)')\n",
    "ax.set_xlabel('Variable Index')\n",
    "ax.set_ylabel('Variable Index')\n",
    "# Mark slow/fast boundary\n",
    "n_slow = len(topology.slow_indices)\n",
    "ax.axhline(y=n_slow-0.5, color='yellow', linewidth=2, label='Slow/Fast boundary')\n",
    "ax.axvline(x=n_slow-0.5, color='yellow', linewidth=2)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Masked Velocity Network\n",
    "\n",
    "The velocity network $v_\\theta(x_t, t, \\text{regime})$ has a special structure:\n",
    "\n",
    "### Architecture\n",
    "- **Input**: State $x_t$, time embedding, regime embedding\n",
    "- **Backbone**: 4-layer Residual MLP with SiLU activation\n",
    "- **Conditioning**: FiLM layers for regime/time modulation\n",
    "- **Masking**: MADE-style masks enforce causal structure\n",
    "\n",
    "### The Causal Mask\n",
    "\n",
    "The mask $M$ ensures the Jacobian $\\partial v / \\partial x$ is lower-triangular:\n",
    "\n",
    "$$M_{ij} = \\begin{cases} 1 & \\text{if } \\text{order}(j) \\leq \\text{order}(i) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "This means:\n",
    "- Slow variables can influence fast variables ✓\n",
    "- Fast variables **cannot** influence slow variables ✗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.network import VelocityNetwork, create_causal_mask\n",
    "\n",
    "# Create a small network for visualization\n",
    "state_dim = 6\n",
    "causal_order = np.array([3, 4, 5, 0, 1, 2])  # Slow (3,4,5) before Fast (0,1,2)\n",
    "\n",
    "# Visualize the causal mask\n",
    "mask = create_causal_mask(dim=state_dim, causal_order=causal_order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot the mask\n",
    "ax = axes[0]\n",
    "im = ax.imshow(mask.numpy(), cmap='Blues')\n",
    "ax.set_xticks(range(state_dim))\n",
    "ax.set_yticks(range(state_dim))\n",
    "ax.set_xticklabels([f'In {i}\\n(order {causal_order[i]})' for i in range(state_dim)])\n",
    "ax.set_yticklabels([f'Out {i}\\n(order {causal_order[i]})' for i in range(state_dim)])\n",
    "ax.set_title('Causal Mask\\n(1 = connection allowed)')\n",
    "\n",
    "for i in range(state_dim):\n",
    "    for j in range(state_dim):\n",
    "        color = 'white' if mask[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, int(mask[i, j].item()), ha='center', va='center', color=color)\n",
    "\n",
    "# Show causal graph\n",
    "ax = axes[1]\n",
    "# Draw nodes\n",
    "slow_x = [1, 2, 3]\n",
    "fast_x = [1, 2, 3]\n",
    "slow_y = [2, 2, 2]\n",
    "fast_y = [0, 0, 0]\n",
    "\n",
    "ax.scatter(slow_x, slow_y, s=500, c='blue', zorder=5)\n",
    "ax.scatter(fast_x, fast_y, s=500, c='red', zorder=5)\n",
    "\n",
    "# Labels\n",
    "for i, x in enumerate(slow_x):\n",
    "    ax.text(x, slow_y[0], f'Slow\\n{i}', ha='center', va='center', color='white', fontweight='bold')\n",
    "for i, x in enumerate(fast_x):\n",
    "    ax.text(x, fast_y[0], f'Fast\\n{i}', ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrows (slow → fast only)\n",
    "for sx in slow_x:\n",
    "    for fx in fast_x:\n",
    "        ax.annotate('', xy=(fx, 0.3), xytext=(sx, 1.7),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green', lw=2, alpha=0.5))\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(-0.5, 2.5)\n",
    "ax.set_title('Causal Graph\\n(Green arrows = allowed influence)')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Fast variables (bottom) receive information from Slow variables (top),\")\n",
    "print(\"but NOT the other way around!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and inspect the velocity network\n",
    "net = VelocityNetwork(\n",
    "    state_dim=topology.X_processed.shape[1],\n",
    "    hidden_dim=128,\n",
    "    n_regimes=topology.n_regimes,\n",
    "    n_layers=4,\n",
    "    causal_order=topology.causal_order\n",
    ")\n",
    "\n",
    "print(\"Velocity Network Architecture:\")\n",
    "print(net)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training with Optimal Transport Paths\n",
    "\n",
    "### The CFM Training Objective\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\left[ \\| v_\\theta(x_t, t, r) - u_t \\|^2 \\right]$$\n",
    "\n",
    "Where:\n",
    "- $t \\sim \\text{Uniform}(0, 1)$\n",
    "- $x_0 \\sim \\mathcal{N}(0, I)$\n",
    "- $x_1 \\sim p_{data}$\n",
    "- $x_t = (1-t) x_0 + t x_1$\n",
    "- $u_t = x_1 - x_0$\n",
    "\n",
    "This is a simple **regression problem** - no ODE solving during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.trainer import FlowMatchingTrainer, TrainingConfig\n",
    "\n",
    "# Configure training\n",
    "config = TrainingConfig(\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    n_epochs=50,  # Reduced for demo\n",
    "    warmup_epochs=5,\n",
    "    validate_every=10\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = FlowMatchingTrainer(net, topology, config)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {config.lr}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Epochs: {config.n_epochs}\")\n",
    "print(f\"  Device: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a single training step\n",
    "print(\"Single Training Step Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get a batch\n",
    "x1_batch, regime_batch = next(iter(trainer.train_loader))\n",
    "x1_batch = x1_batch.to(trainer.device)\n",
    "regime_batch = regime_batch.to(trainer.device)\n",
    "\n",
    "print(f\"\\n1. Sample data batch: x1 shape = {x1_batch.shape}\")\n",
    "\n",
    "# Sample OT path\n",
    "x_t, t, u_t, x_0 = trainer.sample_ot_path(x1_batch, len(x1_batch))\n",
    "\n",
    "print(f\"2. Sample noise: x0 shape = {x_0.shape}\")\n",
    "print(f\"3. Sample time: t shape = {t.shape}, range = [{t.min():.3f}, {t.max():.3f}]\")\n",
    "print(f\"4. Compute interpolation: x_t = (1-t)*x0 + t*x1\")\n",
    "print(f\"5. Target velocity: u_t = x1 - x0, shape = {u_t.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "v_pred = net(x_t, t, regime_batch)\n",
    "print(f\"6. Predict velocity: v_pred shape = {v_pred.shape}\")\n",
    "\n",
    "# Compute loss\n",
    "loss = ((v_pred - u_t) ** 2).mean()\n",
    "print(f\"7. MSE loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "results = trainer.train()\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(results['loss_history'], label='Training Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Curve')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "val_epochs = list(range(config.validate_every-1, config.n_epochs, config.validate_every))\n",
    "ax.plot(val_epochs, results['val_loss_history'], 'o-', label='Validation Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Validation Loss Curve')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal validation loss: {results['final_metrics']['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Scenario Generation and Stress Testing\n",
    "\n",
    "At inference time, we generate samples by solving the ODE:\n",
    "\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t, r), \\quad x(0) \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "### Guided Generation (Stress Testing)\n",
    "\n",
    "To steer a specific variable toward a target value, we modify the velocity:\n",
    "\n",
    "$$v_{guided}(x, t) = v_\\theta(x, t) + \\lambda \\cdot \\frac{\\text{target} - x_i}{1 - t + \\epsilon}$$\n",
    "\n",
    "This \"nudges\" variable $i$ toward the target while the causal mask ensures consistent propagation to other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if torchdiffeq is available\n",
    "try:\n",
    "    from core.solver import ODESolver, ScenarioGenerator\n",
    "    SOLVER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Note: torchdiffeq not installed. Solver demos will be skipped.\")\n",
    "    print(\"Install with: pip install torchdiffeq\")\n",
    "    SOLVER_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOLVER_AVAILABLE:\n",
    "    # Create solver and generator\n",
    "    solver = ODESolver(net, topology)\n",
    "    generator = ScenarioGenerator(solver, topology)\n",
    "    \n",
    "    # Generate baseline scenarios\n",
    "    print(\"Generating baseline scenarios...\")\n",
    "    baseline = generator.baseline(n_scenarios=500)\n",
    "    \n",
    "    # Generate stressed scenarios\n",
    "    print(\"Generating stressed scenarios (3 std shock to first variable)...\")\n",
    "    stressed = generator.shock(\n",
    "        variable=0,  # First fast variable\n",
    "        magnitude=3.0,\n",
    "        n_scenarios=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBaseline shape: {baseline.shape}\")\n",
    "    print(f\"Stressed shape: {stressed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOLVER_AVAILABLE:\n",
    "    # Visualize the difference\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    var_names = topology.variable_names\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= len(var_names):\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        ax.hist(baseline[:, i], bins=30, alpha=0.5, label='Baseline', density=True)\n",
    "        ax.hist(stressed[:, i], bins=30, alpha=0.5, label='Stressed', density=True)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(var_names[i])\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('Baseline vs Stressed Scenario Distributions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nMean Shift (Stressed - Baseline):\")\n",
    "    for i, name in enumerate(var_names[:10]):\n",
    "        shift = stressed[:, i].mean() - baseline[:, i].mean()\n",
    "        print(f\"  {name}: {shift:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Full Pipeline Demonstration\n",
    "\n",
    "Now let's use the high-level `CausalFlowMatcher` API to demonstrate the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import CausalFlowMatcher\n",
    "\n",
    "# Create fresh instance\n",
    "cfm = CausalFlowMatcher(\n",
    "    hidden_dim=128,\n",
    "    n_layers=4,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "print(\"CausalFlowMatcher initialized\")\n",
    "print(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data\n",
    "cfm.fit(\n",
    "    X_raw,\n",
    "    fast_vars=fast_vars,\n",
    "    slow_vars=slow_vars\n",
    ")\n",
    "\n",
    "print(f\"\\nFitted! Detected {cfm.n_regimes} regimes\")\n",
    "print(f\"Variables: {cfm.variable_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "results = cfm.train(\n",
    "    n_epochs=100,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    validate_every=20\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {results['final_metrics']['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOLVER_AVAILABLE:\n",
    "    # Generate and compare scenarios\n",
    "    print(\"Generating scenarios...\")\n",
    "    \n",
    "    baseline = cfm.sample(n_samples=1000)\n",
    "    \n",
    "    # Create stress scenarios\n",
    "    scenarios = {}\n",
    "    scenarios['baseline'] = baseline\n",
    "    scenarios['vix_shock'] = cfm.shock(variable=4, magnitude=3.0, n_samples=1000)  # VIX +3std\n",
    "    scenarios['rate_shock'] = cfm.shock(variable=-2, magnitude=2.0, n_samples=1000)  # Rate +2std\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Pairplot for first two fast variables\n",
    "    ax = axes[0, 0]\n",
    "    for name, data in scenarios.items():\n",
    "        ax.scatter(data[:, 0], data[:, 1], alpha=0.3, s=10, label=name)\n",
    "    ax.set_xlabel(cfm.variable_names[0])\n",
    "    ax.set_ylabel(cfm.variable_names[1])\n",
    "    ax.legend()\n",
    "    ax.set_title('Fast Variable Scatter')\n",
    "    \n",
    "    # Distribution comparison for first variable\n",
    "    ax = axes[0, 1]\n",
    "    for name, data in scenarios.items():\n",
    "        ax.hist(data[:, 0], bins=50, alpha=0.5, label=name, density=True)\n",
    "    ax.set_xlabel(cfm.variable_names[0])\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{cfm.variable_names[0]} Distribution')\n",
    "    \n",
    "    # Box plot comparison\n",
    "    ax = axes[1, 0]\n",
    "    data_for_box = [scenarios['baseline'][:, 0], \n",
    "                    scenarios['vix_shock'][:, 0],\n",
    "                    scenarios['rate_shock'][:, 0]]\n",
    "    ax.boxplot(data_for_box, labels=['Baseline', 'VIX Shock', 'Rate Shock'])\n",
    "    ax.set_ylabel(cfm.variable_names[0])\n",
    "    ax.set_title('Scenario Comparison')\n",
    "    \n",
    "    # Correlation differences\n",
    "    ax = axes[1, 1]\n",
    "    corr_base = np.corrcoef(scenarios['baseline'].T)\n",
    "    corr_vix = np.corrcoef(scenarios['vix_shock'].T)\n",
    "    corr_diff = corr_vix - corr_base\n",
    "    im = ax.imshow(corr_diff[:10, :10], cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
    "    ax.set_title('Correlation Change (VIX Shock - Baseline)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Flow Matching**: Learning to transport noise to data via velocity fields\n",
    "2. **CTree-Lite**: Statistical regime detection using significance testing\n",
    "3. **Data Processing**: Stationarity, normalization, and causal discovery\n",
    "4. **Masked Networks**: Enforcing causal structure in the neural network\n",
    "5. **Training**: Simulation-free CFM with OT paths\n",
    "6. **Generation**: ODE integration with guided stress testing\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- C-CFM generates economically consistent scenarios by respecting causal structure\n",
    "- The mask ensures macro shocks propagate to markets (not vice versa)\n",
    "- Guided generation allows targeted stress testing\n",
    "- The framework is simulation-free during training (efficient!)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with real data from FRED/Yahoo Finance\n",
    "- Experiment with different network architectures\n",
    "- Implement custom regime detection based on domain knowledge\n",
    "- Use for risk management and scenario analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
